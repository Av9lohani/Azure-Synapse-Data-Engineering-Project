{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "mssparkutils.notebook.run('4 - MSSpark Utilities/6 - Mount configuration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## - Accessing the Files from Mountpoint \r\n",
        "## Syntax:\r\n",
        "# synfs:/<jobid>/<mountpoint>/<path>\r\n",
        "# To get JobID - mssparkutils.env.getJobId()\r\n",
        "\r\n",
        "\r\n",
        "job_id = mssparkutils.env.getJobId()\r\n",
        "\r\n",
        "mount_point = 'synfs:/' + job_id + '/lake'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read the dataframe\r\n",
        "\r\n",
        "df = spark.read.format('parquet')\\\r\n",
        "                .load(mount_point+'/DateFunction/*.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_Desc = df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_Desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_final = df.drop('Aggregation_Level','Data_Accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Importing the functions \r\n",
        "\r\n",
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType,DateType\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "\r\n",
        "schema = StructType([\r\n",
        "    StructField('Education_Level',StringType()),\r\n",
        "    StructField('Line_Number',IntegerType()),\r\n",
        "    StructField('Year',IntegerType()),\r\n",
        "    StructField('Month',StringType()),\r\n",
        "    StructField('State',StringType()),\r\n",
        "    StructField('Labor_Force',IntegerType()),\r\n",
        "    StructField('Employed',IntegerType()),\r\n",
        "    StructField('Unemployed',IntegerType()),\r\n",
        "    StructField('Industry',StringType()),\r\n",
        "    StructField('Gender',StringType()),\r\n",
        "    StructField('Date_Inserted',DateType()),\r\n",
        "    StructField('UnEmployed_Rate_Percentage',FloatType()),\r\n",
        "    StructField('Min_Salary_USD',IntegerType()),\r\n",
        "    StructField('Max_Salary_USD',IntegerType()),\r\n",
        "    StructField('dense_rank',IntegerType())\r\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_new = spark.createDataFrame(df_final.rdd,schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_new.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Reading Window FUnction dataframe to test date field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read the dataframe\r\n",
        "\r\n",
        "df_window = spark.read.format('parquet')\\\r\n",
        "                .load(mount_point+'/WindowFunctions/*.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_window.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_window = df_window.drop('Aggregation_Level','Data_Accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_window.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "schema1 = StructType([\r\n",
        "    StructField('Education_Level',StringType()),\r\n",
        "    StructField('Line_Number',IntegerType()),\r\n",
        "    StructField('Year',IntegerType()),\r\n",
        "    StructField('Month',StringType()),\r\n",
        "    StructField('State',StringType()),\r\n",
        "    StructField('Labor_Force',IntegerType()),\r\n",
        "    StructField('Employed',IntegerType()),\r\n",
        "    StructField('Unemployed',IntegerType()),\r\n",
        "    StructField('Industry',StringType()),\r\n",
        "    StructField('Gender',StringType()),\r\n",
        "    StructField('Date_Inserted',StringType()),\r\n",
        "    StructField('UnEmployed_Rate_Percentage',FloatType()),\r\n",
        "    StructField('Min_Salary_USD',IntegerType()),\r\n",
        "    StructField('Max_Salary_USD',IntegerType()),\r\n",
        "    StructField('dense_rank',IntegerType())\r\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_w = spark.createDataFrame(df_window.rdd,schema1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_new.write.format('parquet')\\\r\n",
        "            .mode('overwrite')\\\r\n",
        "            .save(mount_point+'/SchemaManagement/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}