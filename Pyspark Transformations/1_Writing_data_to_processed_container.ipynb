{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "mssparkutils.notebook.run('4 - MSSpark Utilities/6 - Mount configuration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## - Accessing the Files from Mountpoint \r\n",
        "## Syntax:\r\n",
        "# synfs:/<jobid>/<mountpoint>/<path>\r\n",
        "# To get JobID - mssparkutils.env.getJobId()\r\n",
        "\r\n",
        "\r\n",
        "job_id = mssparkutils.env.getJobId()\r\n",
        "\r\n",
        "mount_point = 'synfs:/' + job_id + '/lake'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Reading the previous Schema data to a dataframe\r\n",
        "\r\n",
        "df = spark.read.format('parquet')\\\r\n",
        "                .option('header','true')\\\r\n",
        "                .load(mount_point+'/SchemaManagement/*.parquet')\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.write.format('parquet')\\\r\n",
        "        .mode('overwrite')\\\r\n",
        "        .save('abfss://processed@projectsynapsestorage.dfs.core.windows.net/processed/')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}