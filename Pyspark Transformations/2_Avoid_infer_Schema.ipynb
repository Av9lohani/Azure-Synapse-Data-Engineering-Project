{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Reading the dataframe\r\n",
        "\r\n",
        "df = spark.read.format('csv')\\\r\n",
        "                .option('header','true')\\\r\n",
        "                .option('inferSchema','true')\\\r\n",
        "                .load('abfss://optimization@projectsynapsestorage.dfs.core.windows.net/2. Avoid Inferschema/Unemployment inferschema.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,DateType,FloatType,DoubleType\r\n",
        "\r\n",
        "schema1 = StructType([\r\n",
        "    StructField('Education_Level',StringType()),\r\n",
        "    StructField('Line_Number',IntegerType()),\r\n",
        "    StructField('Year',IntegerType()),\r\n",
        "    StructField('Month',StringType()),\r\n",
        "    StructField('State',StringType()),\r\n",
        "    StructField('Labor_Force',IntegerType()),\r\n",
        "    StructField('Employed',IntegerType()),\r\n",
        "    StructField('Unemployed',IntegerType()),\r\n",
        "    StructField('Industry',StringType()),\r\n",
        "    StructField('Gender',StringType()),\r\n",
        "    StructField('Date_Inserted',StringType()),\r\n",
        "    StructField('Aggregation_level',StringType()),\r\n",
        "    StructField('Data_Accuracy',StringType()),\r\n",
        "    StructField('UnEmployed_Rate_Percentage',DoubleType()),\r\n",
        "    StructField('Min_Salary_USD',IntegerType()),\r\n",
        "    StructField('Max_Salary_USD',IntegerType()),\r\n",
        "    StructField('dense_rank',IntegerType())\r\n",
        "])\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_struct = spark.read.format('csv')\\\r\n",
        "                .option('header','true')\\\r\n",
        "                .schema(schema1)\\\r\n",
        "                .load('abfss://optimization@projectsynapsestorage.dfs.core.windows.net/2. Avoid Inferschema/Unemployment inferschema.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_struct.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df_handled = df_struct.withColumn('Date_Inserted',F.to_date('Date_Inserted','M/d/y'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_handled.printSchema()"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}