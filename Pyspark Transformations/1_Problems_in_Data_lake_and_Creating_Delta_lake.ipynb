{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,DateType,FloatType,DoubleType\r\n",
        "\r\n",
        "schema1 = StructType([\r\n",
        "    StructField('Education_Level',StringType()),\r\n",
        "    StructField('Line_Number',IntegerType()),\r\n",
        "    StructField('Year',IntegerType()),\r\n",
        "    StructField('Month',StringType()),\r\n",
        "    StructField('State',StringType()),\r\n",
        "    StructField('Labor_Force',IntegerType()),\r\n",
        "    StructField('Employed',IntegerType()),\r\n",
        "    StructField('Unemployed',IntegerType()),\r\n",
        "    StructField('Industry',StringType()),\r\n",
        "    StructField('Gender',StringType()),\r\n",
        "    StructField('Date_Inserted',StringType()),\r\n",
        "    StructField('UnEmployed_Rate_Percentage',DoubleType()),\r\n",
        "    StructField('Min_Salary_USD',IntegerType()),\r\n",
        "    StructField('Max_Salary_USD',IntegerType()),\r\n",
        "    StructField('dense_rank',IntegerType())\r\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.format('csv')\\\r\n",
        "               .option('header','true')\\\r\n",
        "               .schema(schema1)\\\r\n",
        "               .load('abfss://delta@projectsynapsestorage.dfs.core.windows.net/Required Files/SchemaManagementDelta.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing into ParquetFolder in parquet format \r\n",
        "\r\n",
        "df.write.format('parquet')\\\r\n",
        "        .mode('overwrite')\\\r\n",
        "        .save('abfss://delta@projectsynapsestorage.dfs.core.windows.net/ParquetFolder/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df1 = spark.read.format('parquet')\\\r\n",
        "               .load('abfss://delta@projectsynapsestorage.dfs.core.windows.net/ParquetFolder/*.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df1.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df1.createOrReplaceTempView('newview')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"\"\"  UPDATE newview SET Education_Level = 'School' WHERE Education_Level = 'High School' \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df1 = df1.filter(\"Education_Level == 'High School'\")\r\n",
        "\r\n",
        "display(df1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing data to Temp folder in parquet format\r\n",
        "\r\n",
        "df1.write.format('parquet')\\\r\n",
        "        .mode('overwrite')\\\r\n",
        "        .save('abfss://delta@projectsynapsestorage.dfs.core.windows.net/Temp/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_temp = spark.read.format('parquet')\\\r\n",
        "               .load('abfss://delta@projectsynapsestorage.dfs.core.windows.net/Temp/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing data to parquet folder in parquet format\r\n",
        "\r\n",
        "df_temp.write.format('parquet')\\\r\n",
        "        .mode('overwrite')\\\r\n",
        "        .save('abfss://delta@projectsynapsestorage.dfs.core.windows.net/ParquetFolder/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_overwritten = spark.read.format('parquet')\\\r\n",
        "               .load('abfss://delta@projectsynapsestorage.dfs.core.windows.net/ParquetFolder/*.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df_overwritten)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing as delta format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing into ParquetFolder in parquet format \r\n",
        "\r\n",
        "df.write.format('delta')\\\r\n",
        "        .mode('overwrite')\\\r\n",
        "        .save('abfss://delta@projectsynapsestorage.dfs.core.windows.net/delta/')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}