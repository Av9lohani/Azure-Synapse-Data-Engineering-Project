{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Reading the dataframe\r\n",
        "\r\n",
        "df = spark.read.format('csv')\\\r\n",
        "                .option('header','true')\\\r\n",
        "                .load('abfss://optimization@projectsynapsestorage.dfs.core.windows.net/3. Cache Persist/cache.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "print('Usign with Column addign a column')\r\n",
        "from pyspark.sql.functions import col\r\n",
        "df_transform = df.withColumn('UnEmployed Rate Percentage',(col('Unemployed')/col('Labor Force'))*100)\r\n",
        "display(df_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_dropped = df_transform.drop('Unemployment Rate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_dropped.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "df_converted = df_dropped.withColumn('Employed',F.col('Employed').cast('Integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from datetime import date,datetime\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "start = datetime.now()\r\n",
        "\r\n",
        "\r\n",
        "# We are just doing a filtering columns where we are having degree\r\n",
        "df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
        "\r\n",
        "# Lets do a sorting on same dataframe\r\n",
        "df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
        "\r\n",
        "# Lets also do some grouping\r\n",
        "df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
        "\r\n",
        "\r\n",
        "end = datetime.now()\r\n",
        "\r\n",
        "duration = end-start\r\n",
        "\r\n",
        "print('Notebook executed in ' + str(duration))\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ==================== PERSIST - MEMORY ONLY -========================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# PySpark Code : MEMORY_ONLY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark import StorageLevel\r\n",
        "\r\n",
        "df_converted.persist(StorageLevel.MEMORY_ONLY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_converted.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scala Code()\r\n",
        "# Persist MEMORY_ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "var df_Scala = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"abfss://optimization@projectsynapsestorage.dfs.core.windows.net/3. Cache Persist/cache.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        " %%spark\r\n",
        "import org.apache.spark.storage.StorageLevel._ \r\n",
        "df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        " %%spark\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
        "\r\n",
        "df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
        "\r\n",
        "df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark \r\n",
        "\r\n",
        "df_Scala.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Using Storage Class - MEMORY_ONLY using pyspark code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### StorageLevel ( <-useDisk->,<-useMemory- >,<-useOffHeap->,<-de-serialized->,<-replication-> )\r\n",
        "### "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_converted.persist(StorageLevel(False,True,False,False,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ==================== PERSIST - MEMORY AND DISK -========================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# PySpark Code <br>\r\n",
        "# Persist - MEMORY AND DISK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Unpersist previous cache or persisted StorageLevel\r\n",
        "\r\n",
        "df_converted.unpersist()\r\n",
        "\r\n",
        "# ------------------------------------ Storage Level -------------------------\r\n",
        "# Applying the  StorageLevel - MEMORY AND DISK\r\n",
        "\r\n",
        "from pyspark import StorageLevel\r\n",
        "\r\n",
        "df_converted.persist(StorageLevel.MEMORY_AND_DISK)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ----------------------------- ACTIONS -------------------------------\r\n",
        "\r\n",
        "from datetime import date,datetime\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "start = datetime.now()\r\n",
        "\r\n",
        "\r\n",
        "# We are just doing a filtering columns where we are having degree\r\n",
        "df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
        "\r\n",
        "# Lets do a sorting on same dataframe\r\n",
        "df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
        "\r\n",
        "# Lets also do some grouping\r\n",
        "df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
        "\r\n",
        "\r\n",
        "end = datetime.now()\r\n",
        "\r\n",
        "duration = end-start\r\n",
        "\r\n",
        "print('Notebook executed in ' + str(duration))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_converted.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scala Code <br>\r\n",
        "# Persist - MEMORY AND DISK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()\r\n",
        "\r\n",
        "// ---------------------------------- Storage Level --------------------\r\n",
        "\r\n",
        "\r\n",
        "import org.apache.spark.storage.StorageLevel._ \r\n",
        "df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK)\r\n",
        "\r\n",
        "// -------------------------------- Actions -------------------\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
        "\r\n",
        "df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
        "\r\n",
        "df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ==================== PERSIST - MEMORY ONLY SER -========================"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scala Code <br>\r\n",
        "# Persist - MEMORY_ONLY_SER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()\r\n",
        "\r\n",
        "// ---------------------------------- Storage Level --------------------\r\n",
        "\r\n",
        "\r\n",
        "import org.apache.spark.storage.StorageLevel._ \r\n",
        "df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER)\r\n",
        "\r\n",
        "// -------------------------------- Actions -------------------\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
        "\r\n",
        "df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
        "\r\n",
        "df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ======================= PERSIST - MEMORY AND DISK SER -================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scala Code\r\n",
        "# Persist - MEMORY AND DISK SER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()\r\n",
        "\r\n",
        "// ---------------------------------- Storage Level --------------------\r\n",
        "\r\n",
        "\r\n",
        "import org.apache.spark.storage.StorageLevel._ \r\n",
        "df_Scala.persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER)\r\n",
        "\r\n",
        "// -------------------------------- Actions -------------------\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
        "\r\n",
        "df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
        "\r\n",
        "df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# ======================= PERSIST - DISK ONLY -================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# PySpark Code <br>\r\n",
        "# Persist - DISK ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Unpersist previous cache or persisted StorageLevel\r\n",
        "\r\n",
        "df_converted.unpersist()\r\n",
        "\r\n",
        "# ------------------------------------ Storage Level -------------------------\r\n",
        "# Applying the  StorageLevel - MEMORY AND DISK\r\n",
        "\r\n",
        "from pyspark import StorageLevel\r\n",
        "\r\n",
        "df_converted.persist(StorageLevel.DISK_ONLY)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ----------------------------- ACTIONS -------------------------------\r\n",
        "\r\n",
        "from datetime import date,datetime\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "start = datetime.now()\r\n",
        "\r\n",
        "\r\n",
        "# We are just doing a filtering columns where we are having degree\r\n",
        "df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
        "\r\n",
        "# Lets do a sorting on same dataframe\r\n",
        "df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
        "\r\n",
        "# Lets also do some grouping\r\n",
        "df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
        "\r\n",
        "\r\n",
        "end = datetime.now()\r\n",
        "\r\n",
        "duration = end-start\r\n",
        "\r\n",
        "print('Notebook executed in ' + str(duration))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_converted.unpersist()\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scala Code <br>\r\n",
        "# Persist - DISK ONLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()\r\n",
        "\r\n",
        "// ---------------------------------- Storage Level --------------------\r\n",
        "\r\n",
        "\r\n",
        "import org.apache.spark.storage.StorageLevel._ \r\n",
        "df_Scala.persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)\r\n",
        "\r\n",
        "// -------------------------------- Actions -------------------\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
        "\r\n",
        "df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
        "\r\n",
        "df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scala Code <br>\r\n",
        "# Persist - OFF HEAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()\r\n",
        "\r\n",
        "// ---------------------------------- Storage Level --------------------\r\n",
        "\r\n",
        "\r\n",
        "import org.apache.spark.storage.StorageLevel._ \r\n",
        "df_Scala.persist(org.apache.spark.storage.StorageLevel.OFF_HEAP)\r\n",
        "\r\n",
        "// -------------------------------- Actions -------------------\r\n",
        "\r\n",
        "import org.apache.spark.sql.functions._\r\n",
        "\r\n",
        "df_Scala.select($\"employed\", $\"unemployed\", ($\"employed\" + $\"unemployed\").alias(\"Total\")).show(5)\r\n",
        "\r\n",
        "df_Scala.filter(col(\"Education Level\").contains(\"degree\")).show(5)\r\n",
        "\r\n",
        "df_Scala.orderBy(col(\"Date Inserted\").desc).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "df_Scala.unpersist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Pyspark Code <br>\r\n",
        "# Perist - MEMORY_ONLY_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Unpersist previous cache or persisted StorageLevel\r\n",
        "\r\n",
        "df_converted.unpersist()\r\n",
        "\r\n",
        "# ------------------------------------ Storage Level -------------------------\r\n",
        "# Applying the  StorageLevel - MEMORY AND DISK\r\n",
        "\r\n",
        "from pyspark import StorageLevel\r\n",
        "\r\n",
        "df_converted.persist(StorageLevel.MEMORY_ONLY_2)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# ----------------------------- ACTIONS -------------------------------\r\n",
        "\r\n",
        "from datetime import date,datetime\r\n",
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "start = datetime.now()\r\n",
        "\r\n",
        "\r\n",
        "# We are just doing a filtering columns where we are having degree\r\n",
        "df_converted.filter(df_converted['Education Level'].contains('degree')).show(n=5)\r\n",
        "\r\n",
        "# Lets do a sorting on same dataframe\r\n",
        "df_converted.orderBy(col('Date Inserted').desc()).show(n=5)\r\n",
        "\r\n",
        "# Lets also do some grouping\r\n",
        "df_converted.groupBy('Gender').agg(F.sum('Employed'), F.avg('Employed')).show(n=5)\r\n",
        "\r\n",
        "\r\n",
        "end = datetime.now()\r\n",
        "\r\n",
        "duration = end-start\r\n",
        "\r\n",
        "print('Notebook executed in ' + str(duration))"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}